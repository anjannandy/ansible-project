---
# ===== USER CREATION AND SSH KEY SETUP (using existing user) =====
- name: Create and configure kubeadmin user on all nodes
  hosts: kubernetes
  become: no
  gather_facts: yes
  strategy: free  # Parallel execution within this step
  vars:
    ansible_user: "{{ hostvars[inventory_hostname]['ansible_user'] | default('anandy') }}"

  tasks:
    - name: Display current connection user
      debug:
        msg: "Connecting to {{ inventory_hostname }} as {{ ansible_user }}"

    - name: Check if kubeadmin user exists
      getent:
        database: passwd
        key: kubeadmin
      register: kubeadmin_exists
      failed_when: false

    - name: Create kubeadmin user if it doesn't exist
      become: yes
      user:
        name: kubeadmin
        groups: sudo,docker
        shell: /bin/bash
        create_home: yes
        append: yes
      when: kubeadmin_exists.failed

    - name: Ensure docker group exists
      become: yes
      group:
        name: docker
        state: present

    - name: Add kubeadmin to docker group (always)
      become: yes
      user:
        name: kubeadmin
        groups: docker
        append: yes

    - name: Set up SSH key for kubeadmin (always run)
      become: yes
      authorized_key:
        user: kubeadmin
        state: present
        key: "{{ lookup('file', '~/.ssh/id_rsa.pub') }}"
        exclusive: no

    - name: Add kubeadmin to sudoers with no password (always run)
      become: yes
      lineinfile:
        path: /etc/sudoers.d/kubeadmin
        line: 'kubeadmin ALL=(ALL) NOPASSWD:ALL'
        create: yes
        mode: '0440'

    - name: Test SSH connection to kubeadmin
      become: no
      vars:
        ansible_user: kubeadmin
      ping:
      register: ssh_test
      ignore_errors: yes

    - name: Display user setup result
      debug:
        msg:
          - "kubeadmin user: {{ 'Already existed' if not kubeadmin_exists.failed else 'Created' }}"
          - "SSH key setup: {{ 'SUCCESS' if ssh_test is succeeded else 'FAILED' }}"
          - "Ready for Kubernetes setup: {{ 'YES' if ssh_test is succeeded else 'NO' }}"

    - name: Fail if SSH test failed
      fail:
        msg: "SSH connection to kubeadmin user failed. Cannot proceed with Kubernetes setup."
      when: ssh_test is failed

# ===== STEP 1: SYSTEM PREPARATION =====
- name: STEP 1 - System Preparation
  hosts: kubernetes
  become: no
  gather_facts: yes
  strategy: free  # All nodes process in parallel
  vars:
    ansible_user: kubeadmin
    kubeadmin_script_dir: "/home/kubeadmin/k8s-scripts"
    kubeadmin_log_dir: "/home/kubeadmin/k8s-setup-logs"
    kubeadmin_state_dir: "/home/kubeadmin/k8s-setup-state"
    max_retries: 3

  tasks:
    - name: Display step 1 start
      debug:
        msg: "üîß STEP 1: Starting system preparation on {{ inventory_hostname }}"

    - name: Create directories for kubeadmin user
      file:
        path: "{{ item }}"
        state: directory
        mode: '0755'
        owner: kubeadmin
        group: kubeadmin
      loop:
        - "{{ kubeadmin_script_dir }}"
        - "{{ kubeadmin_log_dir }}"
        - "{{ kubeadmin_state_dir }}"
      become: yes

    - name: Copy setup scripts to kubeadmin directory
      copy:
        src: "../scripts/k8s/{{ item }}"
        dest: "{{ kubeadmin_script_dir }}/{{ item }}"
        mode: '0755'
        owner: kubeadmin
        group: kubeadmin
      loop:
        - setup_k8s.sh
        - utils.sh
        - run_step.sh
        - cluster_status.sh
      become: yes

    - name: Check if system preparation already completed
      stat:
        path: "{{ kubeadmin_state_dir }}/01_system_prep_completed"
      register: step1_status

    - name: Execute system preparation
      command: "{{ kubeadmin_script_dir }}/run_step.sh system_prep"
      register: step1_result
      retries: "{{ max_retries }}"
      delay: 30
      until: step1_result.rc == 0
      when: not step1_status.stat.exists
      timeout: 1200
      environment:
        HOME: "/home/kubeadmin"
        LOG_FILE: "{{ kubeadmin_log_dir }}/{{ inventory_hostname }}-setup.log"
        STATE_DIR: "{{ kubeadmin_state_dir }}"
        NODE_ROLE: "{{ node_role }}"

    - name: Display system prep results
      debug:
        msg: "‚úÖ STEP 1 - System prep on {{ inventory_hostname }}: {{ 'SKIPPED (already completed)' if step1_status.stat.exists else ('SUCCESS' if step1_result is defined and step1_result.rc == 0 else 'FAILED') }}"

# ===== STEP 2: DOCKER INSTALLATION =====
- name: STEP 2 - Docker Installation
  hosts: kubernetes
  become: no
  gather_facts: no
  strategy: free  # All nodes process in parallel
  vars:
    ansible_user: kubeadmin
    kubeadmin_script_dir: "/home/kubeadmin/k8s-scripts"
    kubeadmin_log_dir: "/home/kubeadmin/k8s-setup-logs"
    kubeadmin_state_dir: "/home/kubeadmin/k8s-setup-state"
    max_retries: 3

  tasks:
    - name: Display step 2 start
      debug:
        msg: "üê≥ STEP 2: Starting Docker installation on {{ inventory_hostname }}"

    - name: Check if Docker installation already completed
      stat:
        path: "{{ kubeadmin_state_dir }}/02_docker_install_completed"
      register: step2_status

    - name: Check Docker service status as fallback
      systemd:
        name: docker
      register: docker_service_status
      failed_when: false
      become: yes
      when: not step2_status.stat.exists

    - name: Install Docker
      command: "{{ kubeadmin_script_dir }}/run_step.sh install_docker"
      register: step2_result
      retries: "{{ max_retries }}"
      delay: 30
      until: step2_result.rc == 0
      when:
        - not step2_status.stat.exists
        - docker_service_status is not defined or docker_service_status.status.ActiveState != 'active'
      timeout: 1200
      environment:
        HOME: "/home/kubeadmin"
        LOG_FILE: "{{ kubeadmin_log_dir }}/{{ inventory_hostname }}-setup.log"
        STATE_DIR: "{{ kubeadmin_state_dir }}"
        NODE_ROLE: "{{ node_role }}"

    - name: Display Docker install results
      debug:
        msg: "‚úÖ STEP 2 - Docker install on {{ inventory_hostname }}: {{ 'SKIPPED (already completed)' if step2_status.stat.exists or (docker_service_status is defined and docker_service_status.status.ActiveState == 'active') else ('SUCCESS' if step2_result is defined and step2_result.rc == 0 else 'FAILED') }}"

# ===== STEP 3: KUBERNETES INSTALLATION =====
- name: STEP 3 - Kubernetes Installation
  hosts: kubernetes
  become: no
  gather_facts: no
  strategy: free  # All nodes process in parallel
  vars:
    ansible_user: kubeadmin
    kubeadmin_script_dir: "/home/kubeadmin/k8s-scripts"
    kubeadmin_log_dir: "/home/kubeadmin/k8s-setup-logs"
    kubeadmin_state_dir: "/home/kubeadmin/k8s-setup-state"
    max_retries: 3

  tasks:
    - name: Display step 3 start
      debug:
        msg: "‚ò∏Ô∏è  STEP 3: Starting Kubernetes installation on {{ inventory_hostname }}"

    - name: Check if Kubernetes installation already completed
      stat:
        path: "{{ kubeadmin_state_dir }}/03_k8s_install_completed"
      register: step3_status

    - name: Check if kubectl is already installed
      command: kubectl version --client --short
      register: kubectl_check
      failed_when: false
      when: not step3_status.stat.exists

    - name: Install Kubernetes components
      command: "{{ kubeadmin_script_dir }}/run_step.sh install_kubernetes"
      register: step3_result
      retries: "{{ max_retries }}"
      delay: 30
      until: step3_result.rc == 0
      when:
        - not step3_status.stat.exists
        - kubectl_check is not defined or kubectl_check.rc != 0
      timeout: 1200
      environment:
        HOME: "/home/kubeadmin"
        LOG_FILE: "{{ kubeadmin_log_dir }}/{{ inventory_hostname }}-setup.log"
        STATE_DIR: "{{ kubeadmin_state_dir }}"
        NODE_ROLE: "{{ node_role }}"

    - name: Display Kubernetes install results
      debug:
        msg: "‚úÖ STEP 3 - Kubernetes install on {{ inventory_hostname }}: {{ 'SKIPPED (already completed)' if step3_status.stat.exists or (kubectl_check is defined and kubectl_check.rc == 0) else ('SUCCESS' if step3_result is defined and step3_result.rc == 0 else 'FAILED') }}"

# ===== STEP 4: SYSTEM CONFIGURATION =====
- name: STEP 4 - System Configuration
  hosts: kubernetes
  become: no
  gather_facts: no
  strategy: free  # All nodes process in parallel
  vars:
    ansible_user: kubeadmin
    kubeadmin_script_dir: "/home/kubeadmin/k8s-scripts"
    kubeadmin_log_dir: "/home/kubeadmin/k8s-setup-logs"
    kubeadmin_state_dir: "/home/kubeadmin/k8s-setup-state"
    max_retries: 3

  tasks:
    - name: Display step 4 start
      debug:
        msg: "‚öôÔ∏è  STEP 4: Starting system configuration on {{ inventory_hostname }}"

    - name: Check if system configuration already completed
      stat:
        path: "{{ kubeadmin_state_dir }}/04_system_config_completed"
      register: step4_status

    - name: Configure system for Kubernetes
      command: "{{ kubeadmin_script_dir }}/run_step.sh configure_system"
      register: step4_result
      retries: "{{ max_retries }}"
      delay: 30
      until: step4_result.rc == 0
      when: not step4_status.stat.exists
      timeout: 600
      environment:
        HOME: "/home/kubeadmin"
        LOG_FILE: "{{ kubeadmin_log_dir }}/{{ inventory_hostname }}-setup.log"
        STATE_DIR: "{{ kubeadmin_state_dir }}"
        NODE_ROLE: "{{ node_role }}"

    - name: Display system config results
      debug:
        msg: "‚úÖ STEP 4 - System config on {{ inventory_hostname }}: {{ 'SKIPPED (already completed)' if step4_status.stat.exists else ('SUCCESS' if step4_result is defined and step4_result.rc == 0 else 'FAILED') }}"

# ===== STEP 5: MASTER INITIALIZATION (Serial - only master node) =====
- name: STEP 5 - Master Initialization
  hosts: master
  become: no
  gather_facts: no
  strategy: linear  # Only one master node, but explicit
  vars:
    ansible_user: kubeadmin
    kubeadmin_script_dir: "/home/kubeadmin/k8s-scripts"
    kubeadmin_log_dir: "/home/kubeadmin/k8s-setup-logs"
    kubeadmin_state_dir: "/home/kubeadmin/k8s-setup-state"
    pod_network_cidr: "10.244.0.0/16"

  tasks:
    - name: Display step 5 start
      debug:
        msg: "üéØ STEP 5: Starting master initialization on {{ inventory_hostname }}"

    - name: Check if master initialization already completed
      stat:
        path: "{{ kubeadmin_state_dir }}/05_master_init_completed"
      register: master_status

    - name: Check if cluster is already initialized
      command: kubectl cluster-info
      register: cluster_check
      failed_when: false
      environment:
        HOME: "/home/kubeadmin"
        KUBECONFIG: "/home/kubeadmin/.kube/config"
      when: not master_status.stat.exists

    - name: Initialize master
      command: "{{ kubeadmin_script_dir }}/run_step.sh initialize_master"
      register: master_result
      when:
        - not master_status.stat.exists
        - cluster_check is not defined or cluster_check.rc != 0
      timeout: 900
      environment:
        HOME: "/home/kubeadmin"
        LOG_FILE: "{{ kubeadmin_log_dir }}/{{ inventory_hostname }}-setup.log"
        STATE_DIR: "{{ kubeadmin_state_dir }}"
        POD_CIDR: "{{ pod_network_cidr }}"
        NODE_ROLE: "master"
        K8S_ADMIN_USER: "kubeadmin"

    - name: Display master init results
      debug:
        msg: "‚úÖ STEP 5 - Master init: {{ 'SKIPPED (already completed)' if master_status.stat.exists or (cluster_check is defined and cluster_check.rc == 0) else ('SUCCESS' if master_result is defined and master_result.rc == 0 else 'FAILED') }}"

# ===== STEP 6: CNI INSTALLATION (Simplified) =====
- name: STEP 6 - CNI Installation
  hosts: master
  become: no
  gather_facts: no
  strategy: linear
  vars:
    ansible_user: kubeadmin

  tasks:
    - name: Display step 6 start
      debug:
        msg: "üåê STEP 6: Starting CNI installation on {{ inventory_hostname }}"

    - name: Check if CNI already installed (state file)
      stat:
        path: "/home/kubeadmin/k8s-setup-state/06_cni_install_completed"
      register: cni_status

    - name: Check if flannel is actually running
      shell: |
        kubectl get pods -n kube-flannel --field-selector=status.phase=Running --no-headers 2>/dev/null | wc -l
      register: flannel_running_count
      failed_when: false
      environment:
        HOME: "/home/kubeadmin"
        KUBECONFIG: "/home/kubeadmin/.kube/config"
      when: not cni_status.stat.exists

    - name: Debug CNI status
      debug:
        msg: |
          CNI Status Check:
          - State file exists: {{ cni_status.stat.exists }}
          - Running flannel pods: {{ flannel_running_count.stdout | default('not checked') }}
          - Will install CNI: {{ not cni_status.stat.exists and (flannel_running_count.stdout | default('0') | int == 0) }}
      when: flannel_running_count is defined

    - name: Install CNI (Flannel)
      command: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
      register: cni_result
      when:
        - not cni_status.stat.exists
        - flannel_running_count is defined
        - flannel_running_count.stdout | int == 0
      timeout: 300
      environment:
        HOME: "/home/kubeadmin"
        KUBECONFIG: "/home/kubeadmin/.kube/config"

    - name: Wait for flannel pods to be ready
      command: kubectl wait --for=condition=ready pod -l app=flannel -n kube-flannel --timeout=300s
      when:
        - cni_result is defined
        - cni_result.changed
      environment:
        HOME: "/home/kubeadmin"
        KUBECONFIG: "/home/kubeadmin/.kube/config"
      ignore_errors: yes

    - name: Verify CNI installation
      shell: kubectl get nodes --no-headers | grep -v NotReady | wc -l
      register: ready_nodes_count
      environment:
        HOME: "/home/kubeadmin"
        KUBECONFIG: "/home/kubeadmin/.kube/config"
      when:
        - cni_result is defined
        - cni_result.changed

    - name: Mark CNI installation as completed
      file:
        path: "/home/kubeadmin/k8s-setup-state/06_cni_install_completed"
        state: touch
        mode: '0644'
      when:
        - cni_result is defined
        - cni_result.changed
        - cni_result.rc == 0

    - name: Display step 6 results
      debug:
        msg: |
          ‚úÖ STEP 6 - CNI Installation Results:
          {{ 'SKIPPED (already completed)' if cni_status.stat.exists else 
             ('SUCCESS - CNI installed' if (cni_result is defined and cni_result.changed) else 
             'SKIPPED (flannel already running)') }}
          

# ===== STEP 7: WORKER JOIN (Parallel within step) =====
- name: STEP 7 - Worker Join
  hosts: workers
  become: no
  gather_facts: no
  strategy: free  # All worker nodes join in parallel
  vars:
    ansible_user: kubeadmin
    kubeadmin_state_dir: "/home/kubeadmin/k8s-setup-state"

  tasks:
    - name: Display step 7 start
      debug:
        msg: "üîó STEP 7: Starting worker join on {{ inventory_hostname }}"

    - name: Check if worker already joined cluster
      stat:
        path: "{{ kubeadmin_state_dir }}/07_worker_join_completed"
      register: worker_status

    - name: Check if node is already part of cluster
      command: kubectl get nodes {{ inventory_hostname }} -o name
      register: node_check
      failed_when: false
      delegate_to: "{{ groups['master'][0] }}"
      environment:
        HOME: "/home/kubeadmin"
        KUBECONFIG: "/home/kubeadmin/.kube/config"
      when: not worker_status.stat.exists

    - name: Get join command from master
      slurp:
        src: "/home/kubeadmin/kubeadm-join-command.sh"
      register: join_command_content
      delegate_to: "{{ groups['master'][0] }}"
      when:
        - not worker_status.stat.exists
        - node_check is defined
        - node_check.rc != 0

    - name: Write join command to worker
      copy:
        content: "{{ join_command_content.content | b64decode }}"
        dest: "/tmp/kubeadm-join-command.sh"
        mode: '0755'
      when:
        - not worker_status.stat.exists
        - node_check is defined
        - node_check.rc != 0
        - join_command_content is defined
        - join_command_content.content is defined

    - name: Execute join command
      command: "sudo bash /tmp/kubeadm-join-command.sh"
      register: worker_join_result
      timeout: 600
      when:
        - not worker_status.stat.exists
        - node_check is defined
        - node_check.rc != 0
        - join_command_content is defined
        - join_command_content.content is defined

    - name: Mark worker join as completed (when join was executed successfully)
      file:
        path: "{{ kubeadmin_state_dir }}/07_worker_join_completed"
        state: touch
        mode: '0644'
      when:
        - worker_join_result is defined
        - worker_join_result.rc is defined
        - worker_join_result.rc == 0

    - name: Mark worker join as completed (when already in cluster)
      file:
        path: "{{ kubeadmin_state_dir }}/07_worker_join_completed"
        state: touch
        mode: '0644'
      when:
        - worker_status.stat.exists or (node_check is defined and node_check.rc == 0)

    - name: Clean up join command file
      file:
        path: "/tmp/kubeadm-join-command.sh"
        state: absent
      when:
        - join_command_content is defined
        - join_command_content.content is defined

    - name: Display worker join results
      debug:
        msg: "‚úÖ STEP 7 - Worker join on {{ inventory_hostname }}: {{ 'SKIPPED (already completed)' if worker_status.stat.exists else ('SKIPPED (already in cluster)' if (node_check is defined and node_check.rc == 0) else ('SUCCESS' if (worker_join_result is defined and worker_join_result.rc is defined and worker_join_result.rc == 0) else 'SKIPPED (conditions not met)')) }}"

# ===== STEP 8: FINAL STATUS (Serial - only master node) =====
- name: STEP 8 - Final Cluster Status
  hosts: master
  become: no
  gather_facts: no
  strategy: linear  # Only master node
  vars:
    ansible_user: kubeadmin
    kubeadmin_script_dir: "/home/kubeadmin/k8s-scripts"

  tasks:
    - name: Display step 8 start
      debug:
        msg: "üìä STEP 8: Getting final cluster status"

    - name: Wait for all nodes to be ready
      command: kubectl get nodes --no-headers
      register: nodes_status
      retries: 12
      delay: 10
      until:
        - nodes_status.stdout_lines | length == groups['kubernetes'] | length
        - "'NotReady' not in nodes_status.stdout"
      environment:
        HOME: "/home/kubeadmin"
        KUBECONFIG: "/home/kubeadmin/.kube/config"

    - name: Get cluster info
      command: kubectl cluster-info --request-timeout=10s
      register: cluster_info_raw
      environment:
        HOME: "/home/kubeadmin"
        KUBECONFIG: "/home/kubeadmin/.kube/config"

    - name: Get nodes info
      command: kubectl get nodes -o wide --no-headers
      register: nodes_info_raw
      environment:
        HOME: "/home/kubeadmin"
        KUBECONFIG: "/home/kubeadmin/.kube/config"

    - name: Display final cluster status
      debug:
        msg: |
          ================================================================
          üéâ KUBERNETES CLUSTER SETUP COMPLETE! üéâ
          ================================================================
          
          üìä CLUSTER SUMMARY:
          ‚Ä¢ Total Nodes: {{ groups['kubernetes'] | length }}
          ‚Ä¢ Master Nodes: {{ groups['master'] | length }}
          ‚Ä¢ Worker Nodes: {{ groups['workers'] | length }}
          
          üñ•Ô∏è  NODES:
          {% for line in nodes_info_raw.stdout_lines %}
          {% set node_info = line.split() %}
          ‚Ä¢ {{ node_info[0] }}: {{ node_info[1] }} ({{ node_info[2] if node_info[2] != '<none>' else 'worker' }})
          {% endfor %}
          
          üîë ACCESS:
          ‚Ä¢ SSH: ssh kubeadmin@{{ ansible_default_ipv4.address }}
          ‚Ä¢ Config: /home/kubeadmin/.kube/config
          
          üöÄ READY FOR WORKLOADS!
          ================================================================